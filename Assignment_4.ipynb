{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow\n",
      "  Using cached tensorflow-2.4.1-cp38-cp38-win_amd64.whl (370.7 MB)\n",
      "Processing c:\\users\\zashr\\appdata\\local\\pip\\cache\\wheels\\a0\\16\\9c\\5473df82468f958445479c59e784896fa24f4a5fc024b0f501\\termcolor-1.1.0-py3-none-any.whl\n",
      "Requirement already satisfied: numpy~=1.19.2 in d:\\users\\zashr\\anaconda3\\lib\\site-packages (from tensorflow) (1.19.2)\n",
      "Collecting grpcio~=1.32.0\n",
      "  Using cached grpcio-1.32.0-cp38-cp38-win_amd64.whl (2.6 MB)\n",
      "Collecting absl-py~=0.10\n",
      "  Using cached absl_py-0.12.0-py3-none-any.whl (129 kB)\n",
      "Requirement already satisfied: h5py~=2.10.0 in d:\\users\\zashr\\anaconda3\\lib\\site-packages (from tensorflow) (2.10.0)\n",
      "Collecting astunparse~=1.6.3\n",
      "  Using cached astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
      "Collecting google-pasta~=0.2\n",
      "  Using cached google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
      "Collecting keras-preprocessing~=1.1.2\n",
      "  Using cached Keras_Preprocessing-1.1.2-py2.py3-none-any.whl (42 kB)\n",
      "Collecting opt-einsum~=3.3.0\n",
      "  Using cached opt_einsum-3.3.0-py3-none-any.whl (65 kB)\n",
      "Collecting protobuf>=3.9.2\n",
      "  Using cached protobuf-3.15.7-py2.py3-none-any.whl (173 kB)\n",
      "Requirement already satisfied: six~=1.15.0 in d:\\users\\zashr\\anaconda3\\lib\\site-packages (from tensorflow) (1.15.0)\n",
      "Requirement already satisfied: wheel~=0.35 in d:\\users\\zashr\\anaconda3\\lib\\site-packages (from tensorflow) (0.35.1)\n",
      "Collecting tensorboard~=2.4\n",
      "  Using cached tensorboard-2.4.1-py3-none-any.whl (10.6 MB)\n",
      "Collecting gast==0.3.3\n",
      "  Using cached gast-0.3.3-py2.py3-none-any.whl (9.7 kB)\n",
      "Collecting tensorflow-estimator<2.5.0,>=2.4.0\n",
      "  Using cached tensorflow_estimator-2.4.0-py2.py3-none-any.whl (462 kB)\n",
      "Requirement already satisfied: typing-extensions~=3.7.4 in d:\\users\\zashr\\anaconda3\\lib\\site-packages (from tensorflow) (3.7.4.3)\n",
      "Collecting flatbuffers~=1.12.0\n",
      "  Using cached flatbuffers-1.12-py2.py3-none-any.whl (15 kB)\n",
      "Processing c:\\users\\zashr\\appdata\\local\\pip\\cache\\wheels\\5f\\fd\\9e\\b6cf5890494cb8ef0b5eaff72e5d55a70fb56316007d6dfe73\\wrapt-1.12.1-py3-none-any.whl\n",
      "Collecting markdown>=2.6.8\n",
      "  Using cached Markdown-3.3.4-py3-none-any.whl (97 kB)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in d:\\users\\zashr\\anaconda3\\lib\\site-packages (from tensorboard~=2.4->tensorflow) (2.24.0)\n",
      "Collecting google-auth<2,>=1.6.3\n",
      "  Using cached google_auth-1.28.0-py2.py3-none-any.whl (136 kB)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in d:\\users\\zashr\\anaconda3\\lib\\site-packages (from tensorboard~=2.4->tensorflow) (1.0.1)\n",
      "Collecting tensorboard-plugin-wit>=1.6.0\n",
      "  Using cached tensorboard_plugin_wit-1.8.0-py3-none-any.whl (781 kB)\n",
      "Collecting google-auth-oauthlib<0.5,>=0.4.1\n",
      "  Using cached google_auth_oauthlib-0.4.4-py2.py3-none-any.whl (18 kB)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in d:\\users\\zashr\\anaconda3\\lib\\site-packages (from tensorboard~=2.4->tensorflow) (50.3.1.post20201107)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in d:\\users\\zashr\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow) (1.25.11)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in d:\\users\\zashr\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow) (2020.6.20)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in d:\\users\\zashr\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow) (3.0.4)\n",
      "Requirement already satisfied: idna<3,>=2.5 in d:\\users\\zashr\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow) (2.10)\n",
      "Collecting pyasn1-modules>=0.2.1\n",
      "  Using cached pyasn1_modules-0.2.8-py2.py3-none-any.whl (155 kB)\n",
      "Collecting cachetools<5.0,>=2.0.0\n",
      "  Using cached cachetools-4.2.1-py3-none-any.whl (12 kB)\n",
      "Collecting rsa<5,>=3.1.4; python_version >= \"3.6\"\n",
      "  Using cached rsa-4.7.2-py3-none-any.whl (34 kB)\n",
      "Collecting requests-oauthlib>=0.7.0\n",
      "  Using cached requests_oauthlib-1.3.0-py2.py3-none-any.whl (23 kB)\n",
      "Collecting pyasn1<0.5.0,>=0.4.6\n",
      "  Using cached pyasn1-0.4.8-py2.py3-none-any.whl (77 kB)\n",
      "Collecting oauthlib>=3.0.0\n",
      "  Using cached oauthlib-3.1.0-py2.py3-none-any.whl (147 kB)\n",
      "Installing collected packages: termcolor, grpcio, absl-py, astunparse, google-pasta, keras-preprocessing, opt-einsum, protobuf, markdown, pyasn1, pyasn1-modules, cachetools, rsa, google-auth, tensorboard-plugin-wit, oauthlib, requests-oauthlib, google-auth-oauthlib, tensorboard, gast, tensorflow-estimator, flatbuffers, wrapt, tensorflow\n",
      "  Attempting uninstall: wrapt\n",
      "    Found existing installation: wrapt 1.11.2\n",
      "    Uninstalling wrapt-1.11.2:\n",
      "      Successfully uninstalled wrapt-1.11.2\n",
      "Successfully installed absl-py-0.12.0 astunparse-1.6.3 cachetools-4.2.1 flatbuffers-1.12 gast-0.3.3 google-auth-1.28.0 google-auth-oauthlib-0.4.4 google-pasta-0.2.0 grpcio-1.32.0 keras-preprocessing-1.1.2 markdown-3.3.4 oauthlib-3.1.0 opt-einsum-3.3.0 protobuf-3.15.7 pyasn1-0.4.8 pyasn1-modules-0.2.8 requests-oauthlib-1.3.0 rsa-4.7.2 tensorboard-2.4.1 tensorboard-plugin-wit-1.8.0 tensorflow-2.4.1 tensorflow-estimator-2.4.0 termcolor-1.1.0 wrapt-1.12.1\n"
     ]
    }
   ],
   "source": [
    "## Imports\n",
    "!pip install tensorflow\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import scipy\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "\u001B[1;32m<ipython-input-3-20b4fa9cb53e>\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[1;31m## Load training data\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m----> 2\u001B[1;33m \u001B[0mtrain_images\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mnp\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mload\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;34m\"train_x.npy\"\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m      3\u001B[0m \u001B[0mtrain_labels\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mnp\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mgenfromtxt\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;34m\"train_y.csv\"\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mdelimiter\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;34m\",\"\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mnames\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;32mTrue\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mdtype\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mint\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;34m\"Label\"\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mD:\\Users\\zashr\\anaconda3\\lib\\site-packages\\numpy\\lib\\npyio.py\u001B[0m in \u001B[0;36mload\u001B[1;34m(file, mmap_mode, allow_pickle, fix_imports, encoding)\u001B[0m\n\u001B[0;32m    437\u001B[0m                 \u001B[1;32mreturn\u001B[0m \u001B[0mformat\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mopen_memmap\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mfile\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mmode\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mmmap_mode\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    438\u001B[0m             \u001B[1;32melse\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 439\u001B[1;33m                 return format.read_array(fid, allow_pickle=allow_pickle,\n\u001B[0m\u001B[0;32m    440\u001B[0m                                          pickle_kwargs=pickle_kwargs)\n\u001B[0;32m    441\u001B[0m         \u001B[1;32melse\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mD:\\Users\\zashr\\anaconda3\\lib\\site-packages\\numpy\\lib\\format.py\u001B[0m in \u001B[0;36mread_array\u001B[1;34m(fp, allow_pickle, pickle_kwargs)\u001B[0m\n\u001B[0;32m    739\u001B[0m         \u001B[1;32mif\u001B[0m \u001B[0misfileobj\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mfp\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    740\u001B[0m             \u001B[1;31m# We can use the fast fromfile() function.\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 741\u001B[1;33m             \u001B[0marray\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mnumpy\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mfromfile\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mfp\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mdtype\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mdtype\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mcount\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mcount\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    742\u001B[0m         \u001B[1;32melse\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    743\u001B[0m             \u001B[1;31m# This is not a real file. We have to read it the\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "## Load training data\n",
    "train_images = np.load(\"train_x.npy\")\n",
    "train_labels = np.genfromtxt(\"train_y.csv\", delimiter=\",\", names=True, dtype=int)[\"Label\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "## Load test data\n",
    "test_images = np.load(\"test_x.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "## Reshape data into single-pixel arrays\n",
    "train_images = train_images.reshape(40000, 128, 128, 1)\n",
    "test_images = test_images.reshape(10000, 128, 128, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "## Initialize data generator and free up the massive npys\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    horizontal_flip=True\n",
    "    # vertical_flip=True,\n",
    "    # validation_split=0.2\n",
    ")\n",
    "train_generator=train_datagen.flow(train_images, train_labels)\n",
    "del train_images\n",
    "del train_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "## Do the same for the test set\n",
    "test_datagen = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    horizontal_flip=True\n",
    "    # vertical_flip=True,\n",
    "    # validation_split=0.2\n",
    ")\n",
    "test_generator=test_datagen.flow(test_images)\n",
    "# del test_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "## Show first batch of images with their label\n",
    "\n",
    "def show_train_image(image_index):\n",
    "    show_image(train_images[image_index])\n",
    "    print(\"Label: \", round(train_labels[image_index]))\n",
    "\n",
    "# def show_image(arr):\n",
    "#     two_d = (np.reshape(arr, (128, 128)) * 255).astype(np.uint8)\n",
    "#     plt.imshow(two_d, interpolation='nearest')\n",
    "#     plt.show()\n",
    "\n",
    "# def show_from_gen(arr):\n",
    "#     plt.imshow((arr * 255).astype(np.uint8), interpolation='nearest')\n",
    "#     plt.show()\n",
    "\n",
    "# for i in range(1):\n",
    "#     arr = next(train_generator)\n",
    "#     for j in range(6):\n",
    "#         show_from_gen(arr[0][j])\n",
    "#         print('Label: ', arr[1][j])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "## Build the model\n",
    "seq = models.Sequential([\n",
    "    layers.Conv2D(32, (3,3), activation=\"relu\", strides=(2,2), input_shape=(128, 128, 1)),\n",
    "    layers.MaxPooling2D((2,2)),\n",
    "    # layers.Dropout(0.2),\n",
    "    layers.Conv2D(32, (3, 3), activation='relu'),\n",
    "    layers.MaxPooling2D((2,2)),\n",
    "    # # layers.Dropout(0.2),\n",
    "    layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "    layers.MaxPooling2D((2,2)),\n",
    "    # layers.Conv2D(128, (3, 3), activation='relu'),\n",
    "    # layers.MaxPooling2D((2,2)),\n",
    "    layers.Flatten(),\n",
    "    # layers.Dropout(0.2),\n",
    "    # layers.Dense(64, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(9e-4)),\n",
    "    layers.Dropout(0.5),\n",
    "    # layers.Dense(128, activation='relu'),\n",
    "    # layers.Dropout(0.5),\n",
    "    layers.Dense(64, activation='relu'),\n",
    "    # # layers.Dropout(0.5),\n",
    "    layers.Dense(10, activation='softmax')\n",
    "])\n",
    "seq.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# seq = models.load_model(\"seq4.keras\")\n",
    "\n",
    "## Compile and fit the mode\n",
    "# l to the training set\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "filename = \"seq6.keras\"\n",
    "\n",
    "# seq.compile(optimizer=Adam(),\n",
    "#            loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "#            metrics=['accuracy'])\n",
    "\n",
    "\n",
    "checkpointing = ModelCheckpoint(filename, monitor='accuracy',\n",
    "                                       verbose=1, save_best_only=True, mode='max')\n",
    "\n",
    "# seq.fit(train_generator,\n",
    "#                         epochs=20,\n",
    "#                         workers=4,\n",
    "#                         callbacks=[checkpointing],\n",
    "#                         shuffle=True,\n",
    "#                     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "## Additional training\n",
    "seq = models.load_model('seq5.keras')\n",
    "seq.pop(); seq.pop(); seq.pop()\n",
    "seq.add(layers.Dense(64, activation='relu', name=\"added_1\"))\n",
    "seq.add(layers.Dropout(0.5))\n",
    "seq.add(layers.Dense(64, activation='relu', name=\"added_2\"))\n",
    "seq.add(layers.Dense(10, activation='softmax', name=\"added_3\"))\n",
    "seq.fit(train_generator,\n",
    "                        epochs=20,\n",
    "                        workers=4,\n",
    "                        callbacks=[checkpointing],\n",
    "                        shuffle=True,\n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "## Generating predictions on the test set\n",
    "\n",
    "pred = models.load_model('seq5.keras')\n",
    "pred.summary()\n",
    "predictions = pred.predict(test_generator, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print(predictions)\n",
    "for i in range(10):\n",
    "    show_image(test_images[i])\n",
    "    print(\"Predicted label: \", np.argmax(predictions[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%% Dump predictions to csv\n"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'np' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "\u001B[1;32m<ipython-input-1-b2068c94ff8d>\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[1;32m----> 1\u001B[1;33m \u001B[0mid\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mnp\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0marange\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;36m10000\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m      2\u001B[0m \u001B[0mcategory\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mnp\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0marray\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m[\u001B[0m\u001B[0mnp\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0margmax\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mpredictions\u001B[0m\u001B[1;33m[\u001B[0m\u001B[0mi\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m)\u001B[0m \u001B[1;32mfor\u001B[0m \u001B[0mi\u001B[0m \u001B[1;32min\u001B[0m \u001B[0mrange\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mlen\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mpredictions\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      3\u001B[0m \u001B[0moutput\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mnp\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mcolumn_stack\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mid\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mcategory\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      4\u001B[0m \u001B[0mprint\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0moutput\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      5\u001B[0m \u001B[0mnp\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0msavetxt\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mfname\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;34m'predictions.csv'\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mX\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0moutput\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mheader\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;34m'Id,Category'\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mfmt\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;34m\"%d,%d\"\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mcomments\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;34m''\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mNameError\u001B[0m: name 'np' is not defined"
     ]
    }
   ],
   "source": [
    "id = np.arange(10000)\n",
    "category = np.array([np.argmax(predictions[i]) for i in range(len(predictions))])\n",
    "output = np.column_stack((id, category))\n",
    "print(output)\n",
    "np.savetxt(fname='predictions.csv', X=output, header='Id,Category', fmt=\"%d,%d\", comments='')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}